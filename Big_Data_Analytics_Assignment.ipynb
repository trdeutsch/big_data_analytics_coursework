{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b0d455d-5938-4966-9ee3-0c2c10f7fb52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3d3357b-5bb1-4718-ba23-87ebc9763ad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries for PySpark\n",
    "# pyspark: Core PySpark library\n",
    "# SparkContext: Entry point for Spark functionality\n",
    "# SparkSession: Entry point for working with DataFrames\n",
    "import pyspark\n",
    "from pyspark import SparkContext \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65c70f5b-6a49-4dda-b2ff-5fc1ca34173f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#sc=SparkContext() #Uncomment this line if you're creating a SparkContext() for the first time\n",
    "\n",
    "# Create a SparkSession using the existing SparkContext\n",
    "# SparkSession is the entry point for DataFrame and SQL functionality\n",
    "ss=SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca252068-c360-4cae-9525-774885389c51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1faf8e18-96a9-432b-9917-416e1bbed03d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: True"
     ]
    }
   ],
   "source": [
    "# Copy and rename the large_csv.gz file to large.csv.gz\n",
    "dbutils.fs.cp('/FileStore/tables/large_csv.gz', '/FileStore/tables/large.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15024aa5-dcec-42e3-bb15-9a2b43b44fdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6dd271e-2264-4fa6-9e83-ec0fe5cb787d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the large.csv.gz file into a DataFrame\n",
    "# header=True indicates that the first row contains column names\n",
    "large=ss.read.csv('/FileStore/tables/large.csv.gz', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc7b2842-967d-4d6b-81d6-524d2f144e6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/dataframe.py:331: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a temporary SQL table named 'large'\n",
    "# This allows us to run SQL queries on the data using SparkSQL\n",
    "large.registerTempTable('large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e05ea86e-eb17-4c75-b98e-33d1f542c36e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92347b54-9fa9-4564-bff6-38ddc5a1d4a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| large|\n",
      "+------+\n",
      "|389639|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute SQL query to count distinct sentences in the 'large' table\n",
    "# The query selects the count of unique sentences and aliases it as 'large'\n",
    "# show() displays the results\n",
    "ss.sql('select count(distinct sentence) large from large').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "424d4875-ad04-4014-a18d-68b5377691e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "148ad75b-cf19-49ff-abbd-844b477dff0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Execute SQL query to select all sentences from the 'large' table\n",
    "# Store the result in large_sentence DataFrame\n",
    "large_sentence=ss.sql('select sentence from large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10d58fa7-8632-4a53-8e55-3fd258986f0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Repartition the DataFrame into a single partition and write it as a CSV file\n",
    "# repartition(1) combines all data into one partition\n",
    "# write.csv writes the data to the specified path\n",
    "# mode='overwrite' overwrites any existing data at that path\n",
    "large_sentence.repartition(1).write.csv('/FileStore/tables/large_sentence', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa76dad6-4ee1-41d9-af9f-fcdbd71bd464",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the CSV file from the specified path into an RDD\n",
    "# textFile() creates an RDD where each element is a line from the file\n",
    "large_sentence=sc.textFile('/FileStore/tables/large_sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e618b508-a162-49a6-8aa3-c9b71d0b9882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c3ee2df-2589-4d67-b510-3c73a5706ea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: [4571, 2499, 562, 528, 426, 413, 382, 381, 348, 335]"
     ]
    }
   ],
   "source": [
    "# Map each sentence to a tuple of (word count, sentence)\n",
    "# Sort by word count in descending order\n",
    "# Map to just get the word counts\n",
    "# Take the top 10 results\n",
    "large_sentence.map(lambda x: (len(x.split(' ')), x)).sortBy(lambda x: x[0], ascending=False).map(lambda x: x[0]).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0d15d85-4b27-4701-8132-2b8ec8b1307a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c2d6b52-4a25-4274-a452-32058b202bbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the average number of bigrams in the large sentence dataset\n",
    "# 1. Map each sentence to count of bigrams (words-1) \n",
    "# 2. Map each count to [count, 1] for averaging\n",
    "# 3. Reduce by summing counts and number of sentences\n",
    "average_number_bigrams_large_sentence=large_sentence.map(lambda x: len(x.split(' '))-1).map(lambda x: [x, 1]).reduce(lambda x, y: [x[0]+y[0], x[1]+y[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63549ae3-82b9-4ee9-ace3-20948f9bce92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "337385a1-4255-429d-bab9-1f9fc07be02a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: 18.036295"
     ]
    }
   ],
   "source": [
    "# Calculate the average by dividing total number of bigrams by total number of sentences\n",
    "# average_number_bigrams_large_sentence[0] contains sum of bigram counts\n",
    "# average_number_bigrams_large_sentence[1] contains total number of sentences\n",
    "average_number_bigrams_large_sentence[0]/average_number_bigrams_large_sentence[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c708f00d-21a6-4c21-9af0-98deb939346c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a116b93-d036-40fe-992b-89b4ae44868c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split each sentence into a list of words to prepare for bigram creation\n",
    "large_word=large_sentence.map(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4297568b-9770-455e-b7ef-278ff2f3a41a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def bigram(word):\n",
    "    # Initialize empty list to store bigrams\n",
    "    my_list=[]\n",
    "    # Check if word list has at least 2 words to form bigrams\n",
    "    if len(word)>=2:\n",
    "        # Iterate through words to create bigrams\n",
    "        for i in range(0, len(word)-1):\n",
    "            # Get next word index\n",
    "            j=i+1\n",
    "            # Create bigram by concatenating current and next word with space\n",
    "            my_list.append(word[i]+' '+word[j])\n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a97d414-4b3c-457f-b7b9-de323d734692",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply bigram function to each sentence to create bigrams\n",
    "# flatMap is used since each sentence produces multiple bigrams that need to be flattened into a single RDD\n",
    "large_bigram=large_word.flatMap(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "669c8e5c-7285-4f9b-86cd-901b7a8383c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c53e38b8-f763-4d27-8ffc-5f2e374c2dca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[16]: [('of the', 76290),\n",
      " ('in the', 54056),\n",
      " ('to the', 25484),\n",
      " ('at the', 21588),\n",
      " ('is a', 19261),\n",
      " ('for the', 17942),\n",
      " ('on the', 16049),\n",
      " ('and the', 15822),\n",
      " ('as a', 13240),\n",
      " ('with the', 11928)]"
     ]
    }
   ],
   "source": [
    "# Map each bigram to a tuple of (bigram, 1)\n",
    "# Reduce by key to count frequency of each bigram\n",
    "# Sort in descending order by frequency\n",
    "# Take top 10 most frequent bigrams\n",
    "large_bigram.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x+y).sortBy(lambda x: x[1], ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7805dfa9-bb14-4722-a3ac-349f07831c8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "056fb1fc-d733-43b8-a2ff-eccefaae3a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the MAGPIE dataset containing idioms from JSON file into a Spark DataFrame\n",
    "magpie=ss.read.json('/FileStore/tables/MAGPIE_unfiltered.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "162bf1c0-7bca-4a5b-83ed-dc9f29dfc47f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register magpie DataFrame as a temporary SQL table named 'magpie' for querying\n",
    "magpie.registerTempTable('magpie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0858d52f-d398-4a66-967d-2a45688eeb5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/dataframe.py:331: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Convert RDD of bigrams to DataFrame with single column 'bigrams'\n",
    "# Map each bigram to a single-element list to create proper structure for DataFrame\n",
    "# Register as temporary table for SQL querying\n",
    "large_bigram.map(lambda x: [x]).toDF(['bigrams']).registerTempTable('large_bigrams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e614091-3f65-46a2-9435-370646799954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3f25ee8-11de-4ed9-9e37-412b71cf3f41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|large|\n",
      "+-----+\n",
      "|   67|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count how many unique bigrams from our text appear in the MAGPIE idioms dataset\n",
    "# Uses SQL to:\n",
    "# 1. Select distinct bigrams from large_bigrams table \n",
    "# 2. Check if each bigram exists in the idioms column of magpie table\n",
    "# 3. Count the total matches\n",
    "ss.sql('select count(distinct bigrams) large from large_bigrams where bigrams in (select idiom from magpie)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cd62287-ec83-4729-aeb1-a171579875d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Question 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "583104d8-c762-4d2a-ab26-47ad08e9ed19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with bigram frequencies:\n",
    "# 1. Map each bigram to a tuple (bigram, 1)\n",
    "# 2. Reduce by key to sum the counts for each bigram\n",
    "# 3. Convert to DataFrame with bigram and frequency columns\n",
    "# 4. Register as temp table for SQL querying\n",
    "large_bigram.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x+y).toDF(['bigrams', 'frequency']).registerTempTable('large_bigrams_frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac619d07-e5e8-4dd2-bfb1-e7aa74083de1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69ba1587-ae18-4574-8189-46f460ef286c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+----+\n",
      "|         bigrams|frequency|rank|\n",
      "+----------------+---------+----+\n",
      "|         2018 to|      174|2500|\n",
      "| Communist Party|      174|2501|\n",
      "|Cup competition.|      174|2502|\n",
      "|      During the|      174|2503|\n",
      "|       a meeting|      174|2504|\n",
      "|         a state|      174|2505|\n",
      "|      a two-year|      174|2506|\n",
      "|           at St|      174|2507|\n",
      "|  been described|      174|2508|\n",
      "|       bishop of|      174|2509|\n",
      "|        chose to|      174|2510|\n",
      "+----------------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query to find bigrams ranked 2500-2510 by frequency (descending) and alphabetically\n",
    "# 1. Inner query: Select all bigrams and frequencies, excluding those that are idioms\n",
    "# 2. Assign dense rank based on frequency (desc) and bigram text (asc) \n",
    "# 3. Outer query: Filter to only show ranks 2500-2510\n",
    "ss.sql('select * from (select *, dense_rank() over(order by frequency desc, bigrams asc) rank from large_bigrams_frequency where bigrams not in (select idiom from magpie)) where rank between 2500 and 2510').show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Big_Data_Analytics_Assignment",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
